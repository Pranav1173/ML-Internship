{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aeade0c-1244-4a5c-ab98-45cb4b5965f8",
   "metadata": {},
   "source": [
    "*Importing the required Dependencies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "187d5dad-428f-4bb8-9bd7-a5c795fe36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e733ec9c-0a99-439c-8d85-69d563c63655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pyaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dce88b-0f15-402d-bd37-eb752e5f4c4d",
   "metadata": {},
   "source": [
    "*Loading both face emotion and voice emotion model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9777866-0ce9-4691-b913-3e54c2ea3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# face emotion detection model\n",
    "json_file_face = open(\"model_architecture.json\", \"r\")\n",
    "model_json_face = json_file_face.read()\n",
    "json_file_face.close()\n",
    "model_face = load_model(\"model_weights.h5\")\n",
    "\n",
    "# voice emotion detection model\n",
    "model_voice = load_model('voice_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4b457f-06be-4499-9ad2-8b0593433c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "face_cascade = cv2.CascadeClassifier(haar_file)\n",
    "\n",
    "chunk_size = 1024\n",
    "format = pyaudio.paInt16\n",
    "channels = 1\n",
    "sample_rate_voice = 44100\n",
    "p = pyaudio.PyAudio()\n",
    "stream_voice = p.open(format=format,\n",
    "                      channels=channels,\n",
    "                      rate=sample_rate_voice,\n",
    "                      input=True,\n",
    "                      frames_per_buffer=chunk_size)\n",
    "\n",
    "# Labels for face emotion detection\n",
    "labels_face = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
    "\n",
    "# Labels for voice emotion detection\n",
    "labels_voice = {0: 'neutral', 1: 'calm', 2: 'happy', 3: 'sad', 4: 'angry', 5: 'fear', 6: 'disgust', 7: 'surprise'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851d4073-82b7-46b3-a6aa-26fb07bcef27",
   "metadata": {},
   "source": [
    "*Real Time Detection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc74e44f-e14a-4367-b334-24e8424f30f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n"
     ]
    }
   ],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "buffer_voice = np.zeros(chunk_size, dtype=np.int16)\n",
    "buffer_duration = 5 \n",
    "\n",
    "while True:\n",
    "    ret, frame = camera.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    # Face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (p, q, r, s) in faces:\n",
    "        face_roi = gray[q:q + s, p:p + r]\n",
    "        cv2.rectangle(frame, (p, q), (p + r, q + s), (255, 0, 0), 2)\n",
    "        face_roi = cv2.resize(face_roi, (48, 48))\n",
    "        img_face = np.array(face_roi).reshape(1, 48, 48, 1) / 255.0\n",
    "        pred_face = model_face.predict(img_face)\n",
    "        emotion_label_face = labels_face[pred_face.argmax()]\n",
    "        cv2.putText(frame, emotion_label_face, (p - 10, q - 10), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2, (0, 0, 255))\n",
    "\n",
    "    # Voice detection\n",
    "    data_voice = np.frombuffer(stream_voice.read(chunk_size), dtype=np.int16)\n",
    "    buffer_voice = np.concatenate((buffer_voice[len(data_voice):], data_voice))\n",
    "\n",
    "    if len(buffer_voice) >= buffer_duration * sample_rate_voice:\n",
    "        # Process the real-time audio\n",
    "        features_voice = extract_realtime_features(buffer_voice, sample_rate_voice)\n",
    "        features_voice = np.expand_dims(features_voice, axis=2)\n",
    "\n",
    "        # Make prediction\n",
    "        pred_voice = model_voice.predict(features_voice)\n",
    "        emotion_label_voice = labels_voice[pred_voice.argmax()]\n",
    "\n",
    "        print(\"Predicted Emotion (Voice): \", emotion_label_voice)\n",
    "\n",
    "    # Display the combined result\n",
    "    cv2.imshow(\"Combined Emotion Recognition \", frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):  # Press 'q' to exit\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "stream_voice.stop_stream()\n",
    "stream_voice.close()\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2a382-06ff-4114-8a82-b36e9f4e1a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
